#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆå¸‚åœºæ•°æ®åˆ†æè„šæœ¬
åˆ†æä»·æ ¼å·®å¼‚ã€åœç‰Œå¤ç‰Œæ¨¡å¼ã€æˆäº¤é‡ä»·æ ¼å…³ç³»ç­‰
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import logging
from pathlib import Path
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.database.connector import JuyuanDB

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('output/logs/comprehensive_market_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class ComprehensiveMarketAnalyzer:
    """ç»¼åˆå¸‚åœºæ•°æ®åˆ†æå™¨"""
    
    def __init__(self, year=2024, sample_size=1000):
        self.year = year
        self.sample_size = sample_size
        self.output_dir = Path(f'output/processed_data/comprehensive_analysis_{year}')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Liberation Sans', 'Noto Sans CJK SC', 'WenQuanYi Micro Hei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # å¿½ç•¥å­—ä½“è­¦å‘Š
        warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
        
    def fetch_all_data(self):
        """è·å–æ‰€æœ‰æ•°æ®ï¼ˆæŒ‰æœˆé‡‡æ ·ï¼‰"""
        logging.info(f"å¼€å§‹è·å–{self.year}å¹´æ•°æ®ï¼ˆæ¯æœˆé‡‡æ ·{self.sample_size}æ¡ï¼‰")
        
        try:
            db = JuyuanDB()
            all_sampled_data = []
            
            for month in range(1, 13):
                start_date = f"{self.year}-{month:02d}-01"
                if month == 12:
                    end_date = f"{self.year}-12-31"
                else:
                    end_date = f"{self.year}-{month+1:02d}-01"
                    end_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
                
                logging.info(f"é‡‡æ · {self.year}å¹´{month}æœˆ æ•°æ® ({start_date} åˆ° {end_date})")
                
                # æŸ¥è¯¢å•æœˆæ•°æ®
                sql = f"""
                SELECT 
                    c.SecuCode,
                    c.SecuMarket,
                    c.ListedSector,
                    a.TradingDay,
                    a.OpenPrice,
                    a.ClosePrice,
                    a.HighPrice,
                    a.LowPrice,
                    a.TurnoverVolume as Volume,
                    a.TurnoverValue as Amount,
                    a.PrevClosePrice,
                    d.Ifsuspend,
                    s.SuspendReason,
                    s.ResumptionDate as ResumeDate
                FROM QT_DailyQuote a
                LEFT JOIN SecuMain c ON a.InnerCode = c.InnerCode
                LEFT JOIN QT_StockPerformance d ON a.InnerCode = d.InnerCode AND a.TradingDay = d.TradingDay
                LEFT JOIN LC_SuspendResumption s ON a.InnerCode = s.InnerCode AND a.TradingDay = s.SuspendDate
                WHERE c.SecuCategory = 1 
                    AND c.ListedState = 1
                    AND c.SecuMarket IN (83, 90)
                    AND a.TradingDay BETWEEN '{start_date}' AND '{end_date}'
                    AND a.OpenPrice IS NOT NULL
                    AND a.ClosePrice IS NOT NULL
                    AND a.HighPrice IS NOT NULL
                    AND a.LowPrice IS NOT NULL
                ORDER BY a.TradingDay, c.SecuCode
                """
                
                month_df = db.read_sql(sql)
                
                if not month_df.empty:
                    # éšæœºé‡‡æ ·
                    sample_size = min(self.sample_size, len(month_df))
                    if len(month_df) > sample_size:
                        sampled_df = month_df.sample(n=sample_size, random_state=42)
                        logging.info(f"{self.year}å¹´{month}æœˆ: æ€»æ•°æ®{len(month_df)}è¡Œï¼Œéšæœºé‡‡æ ·{sample_size}è¡Œ")
                    else:
                        sampled_df = month_df
                        logging.info(f"{self.year}å¹´{month}æœˆ: æ€»æ•°æ®{len(month_df)}è¡Œï¼Œå…¨éƒ¨é‡‡æ ·")
                    
                    # æ·»åŠ é‡‡æ ·æ ‡è¯†
                    sampled_df['SampleYear'] = self.year
                    sampled_df['SampleMonth'] = month
                    sampled_df['SampleType'] = 'Random'
                    
                    all_sampled_data.append(sampled_df)
                else:
                    logging.warning(f"{self.year}å¹´{month}æœˆ: æ— æ•°æ®")
            
            db.close()
            
            if all_sampled_data:
                # åˆå¹¶æ‰€æœ‰é‡‡æ ·æ•°æ®
                df = pd.concat(all_sampled_data, ignore_index=True)
                logging.info(f"é‡‡æ ·æ•°æ®è·å–å®Œæˆï¼Œå…± {len(df)} è¡Œ")
                
                # ä¿å­˜é‡‡æ ·æ•°æ®
                sample_file = self.output_dir / 'sampled_data.csv'
                df.to_csv(sample_file, index=False, encoding='utf-8-sig')
                logging.info(f"é‡‡æ ·æ•°æ®å·²ä¿å­˜åˆ°: {sample_file}")
                
                return df
            else:
                logging.error("æ‰€æœ‰æœˆä»½éƒ½æ²¡æœ‰æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            logging.error(f"è·å–æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def analyze_price_gaps(self, df):
        """åˆ†æä»·æ ¼å·®å¼‚"""
        logging.info("åˆ†æä»·æ ¼å·®å¼‚...")
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_df = df[
            (df['OpenPrice'] > 0) & 
            (df['ClosePrice'] > 0) & 
            (df['HighPrice'] > 0) & 
            (df['LowPrice'] > 0) & 
            (df['Ifsuspend'] != 1)  # æ’é™¤åœç‰Œæ•°æ®
        ].copy()
        
        if valid_df.empty:
            logging.warning("æ— æœ‰æ•ˆæ•°æ®ç”¨äºä»·æ ¼å·®å¼‚åˆ†æ")
            return pd.DataFrame(), {}
        
        # è®¡ç®—ä»·æ ¼å·®å¼‚
        valid_df['MaxOpenClose'] = valid_df[['OpenPrice', 'ClosePrice']].max(axis=1)
        valid_df['MinOpenClose'] = valid_df[['OpenPrice', 'ClosePrice']].min(axis=1)
        valid_df['HighGap'] = valid_df['HighPrice'] - valid_df['MaxOpenClose']
        valid_df['LowGap'] = valid_df['MinOpenClose'] - valid_df['LowPrice']
        
        # è®¡ç®—ç™¾åˆ†æ¯”å·®å¼‚
        valid_df['HighGapPct'] = (valid_df['HighGap'] / valid_df['MaxOpenClose']) * 100
        valid_df['LowGapPct'] = (valid_df['LowGap'] / valid_df['MinOpenClose']) * 100
        
        # è®¡ç®—æˆäº¤é¢è°ƒæ•´å› å­
        valid_df['VolumeCloseValue'] = valid_df['Volume'] * valid_df['ClosePrice']  # æˆäº¤é‡Ã—æ”¶ç›˜ä»·
        valid_df['AmountAdjustFactor'] = (valid_df['Amount'] - valid_df['VolumeCloseValue']) / valid_df['VolumeCloseValue'] * 100  # è°ƒæ•´å› å­ç™¾åˆ†æ¯”
        valid_df['AvgPrice'] = valid_df['Amount'] / valid_df['Volume']  # å®é™…æˆäº¤å‡ä»·
        valid_df['AvgPriceAdjustFactor'] = (valid_df['AvgPrice'] - valid_df['ClosePrice']) / valid_df['ClosePrice'] * 100  # å‡ä»·è°ƒæ•´å› å­ç™¾åˆ†æ¯”
        
        # è®¡ç®—å¼€ç›˜å™ªéŸ³
        valid_df['OpenCloseRatio'] = valid_df['OpenPrice'] / valid_df['PrevClosePrice']
        valid_df['OpenCloseNoise'] = (valid_df['OpenCloseRatio'] - 1) * 100
        
        # æŒ‰æœˆç»Ÿè®¡
        monthly_stats = []
        for month in range(1, 13):
            month_data = valid_df[valid_df['SampleMonth'] == month]
            if len(month_data) > 0:
                stats = {
                    'SampleMonth': month,
                    'æ ·æœ¬æ•°': len(month_data),
                    'æœ€é«˜ä»·å·®å¼‚_å‡å€¼': month_data['HighGap'].mean(),
                    'æœ€é«˜ä»·å·®å¼‚_æ ‡å‡†å·®': month_data['HighGap'].std(),
                    'æœ€é«˜ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼': month_data['HighGapPct'].mean(),
                    'æœ€é«˜ä»·å·®å¼‚ç™¾åˆ†æ¯”_æ ‡å‡†å·®': month_data['HighGapPct'].std(),
                    'æœ€ä½ä»·å·®å¼‚_å‡å€¼': month_data['LowGap'].mean(),
                    'æœ€ä½ä»·å·®å¼‚_æ ‡å‡†å·®': month_data['LowGap'].std(),
                    'æœ€ä½ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼': month_data['LowGapPct'].mean(),
                    'æœ€ä½ä»·å·®å¼‚ç™¾åˆ†æ¯”_æ ‡å‡†å·®': month_data['LowGapPct'].std(),
                    'æˆäº¤é¢è°ƒæ•´å› å­_å‡å€¼': month_data['AmountAdjustFactor'].mean(),
                    'æˆäº¤é¢è°ƒæ•´å› å­_æ ‡å‡†å·®': month_data['AmountAdjustFactor'].std(),
                    'å‡ä»·è°ƒæ•´å› å­_å‡å€¼': month_data['AvgPriceAdjustFactor'].mean(),
                    'å‡ä»·è°ƒæ•´å› å­_æ ‡å‡†å·®': month_data['AvgPriceAdjustFactor'].std(),
                    'å¼€ç›˜å™ªéŸ³_å‡å€¼': month_data['OpenCloseNoise'].mean(),
                    'å¼€ç›˜å™ªéŸ³_æ ‡å‡†å·®': month_data['OpenCloseNoise'].std(),
                    'å¼€ç›˜å™ªéŸ³_ä¸­ä½æ•°': month_data['OpenCloseNoise'].median()
                }
                monthly_stats.append(stats)
        
        monthly_df = pd.DataFrame(monthly_stats)
        
        # æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            'æ ·æœ¬æ•°': len(valid_df),
            'æœ€é«˜ä»·å·®å¼‚_å‡å€¼': valid_df['HighGap'].mean(),
            'æœ€é«˜ä»·å·®å¼‚_æ ‡å‡†å·®': valid_df['HighGap'].std(),
            'æœ€é«˜ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼': valid_df['HighGapPct'].mean(),
            'æœ€é«˜ä»·å·®å¼‚ç™¾åˆ†æ¯”_æ ‡å‡†å·®': valid_df['HighGapPct'].std(),
            'æœ€ä½ä»·å·®å¼‚_å‡å€¼': valid_df['LowGap'].mean(),
            'æœ€ä½ä»·å·®å¼‚_æ ‡å‡†å·®': valid_df['LowGap'].std(),
            'æœ€ä½ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼': valid_df['LowGapPct'].mean(),
            'æœ€ä½ä»·å·®å¼‚ç™¾åˆ†æ¯”_æ ‡å‡†å·®': valid_df['LowGapPct'].std(),
            'æˆäº¤é¢è°ƒæ•´å› å­_å‡å€¼': valid_df['AmountAdjustFactor'].mean(),
            'æˆäº¤é¢è°ƒæ•´å› å­_æ ‡å‡†å·®': valid_df['AmountAdjustFactor'].std(),
            'å‡ä»·è°ƒæ•´å› å­_å‡å€¼': valid_df['AvgPriceAdjustFactor'].mean(),
            'å‡ä»·è°ƒæ•´å› å­_æ ‡å‡†å·®': valid_df['AvgPriceAdjustFactor'].std(),
            'å¼€ç›˜å™ªéŸ³_å‡å€¼': valid_df['OpenCloseNoise'].mean(),
            'å¼€ç›˜å™ªéŸ³_æ ‡å‡†å·®': valid_df['OpenCloseNoise'].std(),
            'å¼€ç›˜å™ªéŸ³_ä¸­ä½æ•°': valid_df['OpenCloseNoise'].median()
        }
        
        # ä¿å­˜ç»“æœ
        monthly_file = self.output_dir / 'price_gaps_monthly.csv'
        monthly_df.to_csv(monthly_file, index=False, encoding='utf-8-sig')
        
        overall_file = self.output_dir / 'price_gaps_overall.csv'
        overall_df = pd.DataFrame(list(overall_stats.items()), columns=['æŒ‡æ ‡', 'å€¼'])
        overall_df.to_csv(overall_file, index=False, encoding='utf-8-sig')
        
        logging.info(f"ä»·æ ¼å·®å¼‚åˆ†æå·²ä¿å­˜åˆ°: {monthly_file} å’Œ {overall_file}")
        
        return monthly_df, overall_stats
    
    def analyze_suspend_resume_patterns(self, df):
        """åˆ†æåœç‰Œå¤ç‰Œæ¨¡å¼ï¼ˆåŸºäºä¸­å›½äº¤æ˜“æ—¥å†ï¼‰"""
        logging.info("åˆ†æåœç‰Œå¤ç‰Œæ¨¡å¼ï¼ˆåŸºäºäº¤æ˜“æ—¥å†ï¼‰...")
        
        # è¿‡æ»¤åœç‰Œæ•°æ®
        suspend_df = df[df['Ifsuspend'] == 1].copy()
        
        if suspend_df.empty:
            logging.warning("æ— åœç‰Œæ•°æ®")
            return pd.DataFrame(), pd.DataFrame()
        
        # åˆ†æå¤ç‰Œæ—¶é—´ï¼ˆåŸºäºäº¤æ˜“æ—¥å†ï¼‰
        resume_patterns = []
        db = JuyuanDB()
        
        for _, row in suspend_df.iterrows():
            if pd.notna(row['ResumeDate']) and pd.notna(row['TradingDay']):
                try:
                    suspend_date = pd.to_datetime(row['TradingDay'])
                    resume_date = pd.to_datetime(row['ResumeDate'])
                    
                    # ä½¿ç”¨äº¤æ˜“æ—¥å†è®¡ç®—åœç‰Œå¤©æ•°
                    suspend_trading_days = self.get_trading_days_between(
                        suspend_date.strftime('%Y-%m-%d'), 
                        resume_date.strftime('%Y-%m-%d'), 
                        db
                    )
                    
                    if suspend_trading_days >= 0:
                        resume_patterns.append({
                            'SecuCode': row['SecuCode'],
                            'TradingDay': row['TradingDay'],
                            'ResumeDate': row['ResumeDate'],
                            'SuspendTradingDays': suspend_trading_days,  # äº¤æ˜“æ—¥å¤©æ•°
                            'SuspendNaturalDays': (resume_date - suspend_date).days,  # è‡ªç„¶æ—¥å¤©æ•°
                            'SuspendReason': row.get('SuspendReason', ''),
                            'SampleMonth': row['SampleMonth']
                        })
                except Exception as e:
                    logging.warning(f"è®¡ç®—åœç‰Œå¤©æ•°å¤±è´¥: {row['SecuCode']} {row['TradingDay']} - {str(e)}")
                    continue
        
        db.close()
        
        if not resume_patterns:
            logging.warning("æ— æ³•è®¡ç®—åœç‰Œå¤©æ•°")
            return pd.DataFrame(), pd.DataFrame()
        
        patterns_df = pd.DataFrame(resume_patterns)
        
        # ç»Ÿè®¡åœç‰Œäº¤æ˜“æ—¥åˆ†å¸ƒ
        suspend_distribution = patterns_df['SuspendTradingDays'].value_counts().sort_index()
        distribution_stats = []
        
        for days, count in suspend_distribution.items():
            distribution_stats.append({
                'åœç‰Œäº¤æ˜“æ—¥æ•°': days,
                'è‚¡ç¥¨æ•°': count,
                'å æ¯”(%)': count / len(patterns_df) * 100
            })
        
        distribution_df = pd.DataFrame(distribution_stats)
        
        # æŒ‰æœˆç»Ÿè®¡åœç‰Œæƒ…å†µ
        monthly_suspend = patterns_df.groupby('SampleMonth').agg({
            'SuspendTradingDays': ['count', 'mean', 'std', 'min', 'max'],
            'SuspendNaturalDays': ['mean', 'std']
        }).round(2)
        
        # é‡å‘½ååˆ—
        monthly_suspend.columns = [
            'åœç‰Œäº‹ä»¶æ•°', 'å¹³å‡åœç‰Œäº¤æ˜“æ—¥', 'åœç‰Œäº¤æ˜“æ—¥æ ‡å‡†å·®', 'æœ€çŸ­åœç‰Œäº¤æ˜“æ—¥', 'æœ€é•¿åœç‰Œäº¤æ˜“æ—¥',
            'å¹³å‡åœç‰Œè‡ªç„¶æ—¥', 'åœç‰Œè‡ªç„¶æ—¥æ ‡å‡†å·®'
        ]
        monthly_suspend = monthly_suspend.reset_index()
        
        # ä¿å­˜ç»“æœ
        patterns_file = self.output_dir / 'suspend_patterns.csv'
        patterns_df.to_csv(patterns_file, index=False, encoding='utf-8-sig')
        
        distribution_file = self.output_dir / 'resume_distribution.csv'
        distribution_df.to_csv(distribution_file, index=False, encoding='utf-8-sig')
        
        monthly_file = self.output_dir / 'monthly_suspend_stats.csv'
        monthly_suspend.to_csv(monthly_file, index=False, encoding='utf-8-sig')
        
        logging.info(f"åœç‰Œå¤ç‰Œåˆ†æå·²ä¿å­˜åˆ°: {patterns_file}, {distribution_file}, {monthly_file}")
        
        return distribution_df, monthly_suspend
    
    def get_trading_days_between(self, start_date, end_date, db):
        """è®¡ç®—ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„äº¤æ˜“æ—¥æ•°é‡
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            db: æ•°æ®åº“è¿æ¥
            
        Returns:
            int: äº¤æ˜“æ—¥æ•°é‡
        """
        try:
            # ä½¿ç”¨QT_TradingDayNewè¡¨è®¡ç®—äº¤æ˜“æ—¥æ•°é‡
            sql = f"""
            SELECT COUNT(*) as trading_days
            FROM QT_TradingDayNew
            WHERE TradingDate BETWEEN '{start_date}' AND '{end_date}'
                AND SecuMarket IN (83, 90)  -- ä¸­å›½è‚¡å¸‚ï¼ˆæ·±äº¤æ‰€ã€ä¸Šäº¤æ‰€ï¼‰
                AND IfTradingDay = 1  -- æ˜¯äº¤æ˜“æ—¥
            """
            result = db.read_sql(sql)
            trading_days = result.iloc[0]['trading_days'] if not result.empty else 0
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            natural_days = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days + 1
            logging.debug(f"äº¤æ˜“æ—¥è®¡ç®—: {start_date} åˆ° {end_date}")
            logging.debug(f"  è‡ªç„¶æ—¥æ•°: {natural_days}")
            logging.debug(f"  äº¤æ˜“æ—¥æ•°: {trading_days}")
            logging.debug(f"  å·®å¼‚(èŠ‚å‡æ—¥+å‘¨æœ«): {natural_days - trading_days}")
            
            return trading_days
            
        except Exception as e:
            logging.warning(f"è®¡ç®—äº¤æ˜“æ—¥æ•°é‡å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨è‡ªç„¶æ—¥è®¡ç®—")
            # å›é€€åˆ°è‡ªç„¶æ—¥è®¡ç®—
            natural_days = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days + 1
            return natural_days
    
    def analyze_volume_price_relationship(self, df):
        """åˆ†ææˆäº¤é‡ä»·æ ¼å…³ç³»"""
        logging.info("åˆ†ææˆäº¤é‡ä»·æ ¼å…³ç³»...")
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_df = df[
            (df['Volume'] > 0) & 
            (df['OpenPrice'] > 0) & 
            (df['ClosePrice'] > 0) & 
            (df['Ifsuspend'] != 1)
        ].copy()
        
        if valid_df.empty:
            logging.warning("æ— æœ‰æ•ˆæ•°æ®ç”¨äºæˆäº¤é‡ä»·æ ¼åˆ†æ")
            return pd.DataFrame(), {}
        
        # è®¡ç®—ä»·æ ¼å˜åŒ–
        valid_df['PriceChange'] = valid_df['ClosePrice'] - valid_df['OpenPrice']
        valid_df['PriceChangePct'] = (valid_df['PriceChange'] / valid_df['OpenPrice']) * 100
        
        # è®¡ç®—æˆäº¤é‡å˜åŒ–ï¼ˆå¦‚æœæœ‰å‰ä¸€æ—¥æ•°æ®ï¼‰
        if 'PrevClosePrice' in valid_df.columns:
            valid_df['VolumeChange'] = valid_df['Volume'] - valid_df['Volume'].shift(1)
            valid_df['VolumeChangePct'] = (valid_df['VolumeChange'] / valid_df['Volume'].shift(1)) * 100
        else:
            valid_df['VolumeChangePct'] = 0
        
        # æŒ‰æœˆç»Ÿè®¡
        monthly_stats = []
        for month in range(1, 13):
            month_data = valid_df[valid_df['SampleMonth'] == month]
            if len(month_data) > 0:
                stats = {
                    'SampleMonth': month,
                    'æ ·æœ¬æ•°': len(month_data),
                    'æˆäº¤é‡_å‡å€¼': month_data['Volume'].mean(),
                    'æˆäº¤é‡_æ ‡å‡†å·®': month_data['Volume'].std(),
                    'æˆäº¤é‡_ä¸­ä½æ•°': month_data['Volume'].median(),
                    'ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼': month_data['PriceChangePct'].mean(),
                    'ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_æ ‡å‡†å·®': month_data['PriceChangePct'].std(),
                    'ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_ä¸­ä½æ•°': month_data['PriceChangePct'].median(),
                    'æˆäº¤é‡å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼': month_data['VolumeChangePct'].mean(),
                    'æˆäº¤é‡å˜åŒ–ç™¾åˆ†æ¯”_æ ‡å‡†å·®': month_data['VolumeChangePct'].std()
                }
                monthly_stats.append(stats)
        
        monthly_df = pd.DataFrame(monthly_stats)
        
        # æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            'æ ·æœ¬æ•°': len(valid_df),
            'æˆäº¤é‡_å‡å€¼': valid_df['Volume'].mean(),
            'æˆäº¤é‡_æ ‡å‡†å·®': valid_df['Volume'].std(),
            'æˆäº¤é‡_ä¸­ä½æ•°': valid_df['Volume'].median(),
            'ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼': valid_df['PriceChangePct'].mean(),
            'ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_æ ‡å‡†å·®': valid_df['PriceChangePct'].std(),
            'ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_ä¸­ä½æ•°': valid_df['PriceChangePct'].median(),
            'æˆäº¤é‡å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼': valid_df['VolumeChangePct'].mean(),
            'æˆäº¤é‡å˜åŒ–ç™¾åˆ†æ¯”_æ ‡å‡†å·®': valid_df['VolumeChangePct'].std()
        }
        
        # ä¿å­˜ç»“æœ
        monthly_file = self.output_dir / 'volume_price_monthly.csv'
        monthly_df.to_csv(monthly_file, index=False, encoding='utf-8-sig')
        
        overall_file = self.output_dir / 'volume_price_overall.csv'
        overall_df = pd.DataFrame(list(overall_stats.items()), columns=['æŒ‡æ ‡', 'å€¼'])
        overall_df.to_csv(overall_file, index=False, encoding='utf-8-sig')
        
        logging.info(f"æˆäº¤é‡ä»·æ ¼åˆ†æå·²ä¿å­˜åˆ°: {monthly_file} å’Œ {overall_file}")
        
        return monthly_df, overall_stats
    
    def create_visualizations(self, price_gaps_monthly, resume_distribution, volume_price_monthly):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        logging.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'Comprehensive Market Analysis {self.year}', fontsize=16, fontweight='bold')
        
        # 1. ä»·æ ¼å·®å¼‚åˆ†æ
        if not price_gaps_monthly.empty:
            months = price_gaps_monthly['SampleMonth']
            
            axes[0, 0].plot(months, price_gaps_monthly['æœ€é«˜ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼'], 'o-', label='High Price Gap', color='red')
            axes[0, 0].plot(months, price_gaps_monthly['æœ€ä½ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼'], 's-', label='Low Price Gap', color='blue')
            axes[0, 0].set_xlabel('Month')
            axes[0, 0].set_ylabel('Gap Percentage (%)')
            axes[0, 0].set_title('Price Gap Monthly Trend')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_xticks(range(1, 13))
        
        # 2. å¼€ç›˜å™ªéŸ³åˆ†æ
        if not price_gaps_monthly.empty:
            axes[0, 1].plot(months, price_gaps_monthly['å¼€ç›˜å™ªéŸ³_å‡å€¼'], 'o-', color='green', label='Opening Noise Mean')
            axes[0, 1].fill_between(months, 
                                   price_gaps_monthly['å¼€ç›˜å™ªéŸ³_å‡å€¼'] - price_gaps_monthly['å¼€ç›˜å™ªéŸ³_æ ‡å‡†å·®'],
                                   price_gaps_monthly['å¼€ç›˜å™ªéŸ³_å‡å€¼'] + price_gaps_monthly['å¼€ç›˜å™ªéŸ³_æ ‡å‡†å·®'],
                                   alpha=0.3, color='green', label='Â±1 Std Dev')
            axes[0, 1].set_xlabel('Month')
            axes[0, 1].set_ylabel('Opening Noise (%)')
            axes[0, 1].set_title('Opening Price Noise Monthly Trend')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].set_xticks(range(1, 13))
        
        # 3. åœç‰Œå¤ç‰Œåˆ†å¸ƒ
        if not resume_distribution.empty:
            days = resume_distribution['åœç‰Œäº¤æ˜“æ—¥æ•°']
            counts = resume_distribution['è‚¡ç¥¨æ•°']
            
            axes[1, 0].bar(days, counts, color='orange', alpha=0.7)
            axes[1, 0].set_xlabel('Suspension Trading Days')
            axes[1, 0].set_ylabel('Number of Stocks')
            axes[1, 0].set_title('Suspension Duration Distribution')
            axes[1, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (day, count) in enumerate(zip(days, counts)):
                axes[1, 0].text(day, count + max(counts)*0.01, f'{count}', ha='center', va='bottom')
        
        # 4. æˆäº¤é‡ä»·æ ¼å…³ç³»
        if not volume_price_monthly.empty:
            months = volume_price_monthly['SampleMonth']
            
            ax1 = axes[1, 1]
            ax2 = ax1.twinx()
            
            # æˆäº¤é‡å˜åŒ–
            line1 = ax1.plot(months, volume_price_monthly['æˆäº¤é‡å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼'], 'o-', color='purple', label='Volume Change')
            ax1.set_xlabel('Month')
            ax1.set_ylabel('Volume Change (%)', color='purple')
            ax1.tick_params(axis='y', labelcolor='purple')
            
            # ä»·æ ¼å˜åŒ–
            line2 = ax2.plot(months, volume_price_monthly['ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼'], 's-', color='brown', label='Price Change')
            ax2.set_ylabel('Price Change (%)', color='brown')
            ax2.tick_params(axis='y', labelcolor='brown')
            
            ax1.set_title('Volume vs Price Change Relationship')
            ax1.grid(True, alpha=0.3)
            ax1.set_xticks(range(1, 13))
            
            # åˆå¹¶å›¾ä¾‹
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax1.legend(lines, labels, loc='upper left')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_file = self.output_dir / 'comprehensive_analysis.png'
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        logging.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_file}")
        
        plt.show()
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logging.info(f"å¼€å§‹{self.year}å¹´ç»¼åˆå¸‚åœºæ•°æ®åˆ†æ")
        
        # 1. è·å–æ•°æ®
        df = self.fetch_all_data()
        if df.empty:
            logging.error("æ— æ³•è·å–æ•°æ®ï¼Œåˆ†æç»ˆæ­¢")
            return
        
        # 2. åˆ†æä»·æ ¼å·®å¼‚
        price_gaps_monthly, price_gaps_overall = self.analyze_price_gaps(df)
        
        # 3. åˆ†æåœç‰Œå¤ç‰Œæ¨¡å¼
        resume_distribution, monthly_suspend = self.analyze_suspend_resume_patterns(df)
        
        # 4. åˆ†ææˆäº¤é‡ä»·æ ¼å…³ç³»
        volume_price_monthly, volume_price_overall = self.analyze_volume_price_relationship(df)
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(price_gaps_monthly, resume_distribution, volume_price_monthly)
        
        # 6. è¾“å‡ºæ€»ç»“
        self.print_summary(price_gaps_overall, resume_distribution, volume_price_overall)
        
        logging.info("åˆ†æå®Œæˆ")
    
    def print_summary(self, price_gaps_overall, resume_distribution, volume_price_overall):
        """æ‰“å°åˆ†ææ€»ç»“"""
        print("\n" + "="*80)
        print(f"{self.year}å¹´ç»¼åˆå¸‚åœºæ•°æ®åˆ†ææ€»ç»“")
        print("="*80)
        
        print(f"\nğŸ“Š ä»·æ ¼å·®å¼‚åˆ†æ:")
        print(f"  æœ€é«˜ä»·å·®å¼‚: å¹³å‡ {price_gaps_overall['æœ€é«˜ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼']:.2f}%")
        print(f"  æœ€ä½ä»·å·®å¼‚: å¹³å‡ {price_gaps_overall['æœ€ä½ä»·å·®å¼‚ç™¾åˆ†æ¯”_å‡å€¼']:.2f}%")
        print(f"  å¼€ç›˜å™ªéŸ³: å¹³å‡ {price_gaps_overall['å¼€ç›˜å™ªéŸ³_å‡å€¼']:.2f}% (æ ‡å‡†å·®: {price_gaps_overall['å¼€ç›˜å™ªéŸ³_æ ‡å‡†å·®']:.2f}%)")
        print(f"  æˆäº¤é¢è°ƒæ•´å› å­: å¹³å‡ {price_gaps_overall['æˆäº¤é¢è°ƒæ•´å› å­_å‡å€¼']:.2f}% (æ ‡å‡†å·®: {price_gaps_overall['æˆäº¤é¢è°ƒæ•´å› å­_æ ‡å‡†å·®']:.2f}%)")
        print(f"  å‡ä»·è°ƒæ•´å› å­: å¹³å‡ {price_gaps_overall['å‡ä»·è°ƒæ•´å› å­_å‡å€¼']:.2f}% (æ ‡å‡†å·®: {price_gaps_overall['å‡ä»·è°ƒæ•´å› å­_æ ‡å‡†å·®']:.2f}%)")
        
        print(f"\nğŸ“ˆ åœç‰Œå¤ç‰Œåˆ†æ:")
        if not resume_distribution.empty:
            total_suspend = resume_distribution['è‚¡ç¥¨æ•°'].sum()
            one_day = resume_distribution[resume_distribution['åœç‰Œäº¤æ˜“æ—¥æ•°'] == 1]['è‚¡ç¥¨æ•°'].iloc[0] if len(resume_distribution[resume_distribution['åœç‰Œäº¤æ˜“æ—¥æ•°'] == 1]) > 0 else 0
            two_day = resume_distribution[resume_distribution['åœç‰Œäº¤æ˜“æ—¥æ•°'] == 2]['è‚¡ç¥¨æ•°'].iloc[0] if len(resume_distribution[resume_distribution['åœç‰Œäº¤æ˜“æ—¥æ•°'] == 2]) > 0 else 0
            
            print(f"  æ€»åœç‰Œäº‹ä»¶: {total_suspend} æ¬¡")
            print(f"  ç¬¬ä¸€å¤©å¤ç‰Œ: {one_day} æ¬¡ ({one_day/total_suspend*100:.1f}%)")
            print(f"  ç¬¬äºŒå¤©å¤ç‰Œ: {two_day} æ¬¡ ({two_day/total_suspend*100:.1f}%)")
            print(f"  å…¶ä»–å¤©æ•°å¤ç‰Œ: {total_suspend - one_day - two_day} æ¬¡ ({(total_suspend - one_day - two_day)/total_suspend*100:.1f}%)")
        else:
            print("  æ— åœç‰Œæ•°æ®")
        
        print(f"\nğŸ’° æˆäº¤é‡ä»·æ ¼åˆ†æ:")
        print(f"  å¹³å‡æˆäº¤é‡: {volume_price_overall['æˆäº¤é‡_å‡å€¼']:.0f}")
        print(f"  å¹³å‡ä»·æ ¼å˜åŒ–: {volume_price_overall['ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_å‡å€¼']:.2f}% (æ ‡å‡†å·®: {volume_price_overall['ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”_æ ‡å‡†å·®']:.2f}%)")
        
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
        print(f"  æ•°æ®ç›®å½•: {self.output_dir}")
        print(f"  åˆ†æå›¾è¡¨: {self.output_dir / 'comprehensive_analysis.png'}")
        
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç»¼åˆå¸‚åœºæ•°æ®åˆ†æ')
    parser.add_argument('--year', type=int, default=2024, help='åˆ†æå¹´ä»½')
    parser.add_argument('--sample-size', type=int, default=1000, help='æ¯æœˆé‡‡æ ·æ•°é‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = ComprehensiveMarketAnalyzer(
        year=args.year,
        sample_size=args.sample_size
    )
    
    analyzer.run_analysis()


if __name__ == '__main__':
    main()
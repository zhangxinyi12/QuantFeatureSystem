#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡åŒ–ç‰¹å¾ç³»ç»Ÿä¸»ç¨‹åºå…¥å£
ç»Ÿä¸€çš„æ•°æ®å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€ç›‘æ§å’Œåˆ†æç³»ç»Ÿ
"""

import sys
import os
import logging.config
import argparse
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®
try:
    from config.settings import DATA_CONFIG, OUTPUT_CONFIG, LOGGING_CONFIG
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    DATA_CONFIG = {
        'default_start_date': '2024-01-01',
        'default_end_date': '2024-12-31',
        'market_codes': [1, 2],
        'output_format': 'parquet',
        'chunk_size': 10000
    }
    OUTPUT_CONFIG = {
        'processed_data_dir': Path('output/processed_data'),
        'reports_dir': Path('output/reports'),
        'logs_dir': Path('logs')
    }

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from database.connector import JuyuanDB
    from database.queries import StockQueries
    from processing.timeseries import TimeSeriesProcessor
    from processing.feature_engine.price_features import QuantFeatureEngine
    from utils.memory_utils import MemoryManager, DataFrameChunker, MemoryMonitor
    from utils.monitoring import SystemMonitor
    from utils.data_validator import DataValidator
    from utils.report_generator import ReportGenerator
    from utils.drift_detection import DriftDetector
    from utils.date_utils import TradingCalendar
except ImportError as e:
    print(f"è­¦å‘Š: æŸäº›æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("å°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½æ¨¡å¼")

class QuantFeatureSystem:
    """é‡åŒ–ç‰¹å¾ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, start_date=None, end_date=None):
        """åˆå§‹åŒ–ç³»ç»Ÿ
        
        Args:
            start_date: å›æµ‹å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: å›æµ‹ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        self.start_date = start_date or DATA_CONFIG.get('default_start_date', '2024-01-01')
        self.end_date = end_date or DATA_CONFIG.get('default_end_date', '2024-12-31')
        
        self.setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.create_output_directories()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.init_components()
        
        self.logger.info(f"å›æµ‹æ—¶é—´èŒƒå›´: {self.start_date} åˆ° {self.end_date}")
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_dir = OUTPUT_CONFIG.get('logs_dir', Path('logs'))
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"quant_feature_system_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        directories = [
            OUTPUT_CONFIG.get('processed_data_dir', Path('output/processed_data')),
            OUTPUT_CONFIG.get('reports_dir', Path('output/reports')),
            OUTPUT_CONFIG.get('logs_dir', Path('logs'))
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
    def init_components(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            self.memory_manager = MemoryManager()
            self.memory_monitor = MemoryMonitor()
            self.system_monitor = SystemMonitor()
            self.validator = DataValidator()
            self.report_generator = ReportGenerator()
            self.drift_detector = DriftDetector()
            self.calendar = TradingCalendar()
            
            # åˆå§‹åŒ–ç‰¹å¾å¼•æ“
            self.volume_engine = QuantFeatureEngine()
            
            self.logger.info("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"éƒ¨åˆ†ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            
    def process_stock_data(self, args, use_ssh_tunnel=False):
        """å¤„ç†è‚¡ç¥¨æ•°æ®ï¼ˆä»æ•°æ®åº“æŸ¥è¯¢åˆ°ç‰¹å¾ç”Ÿæˆï¼‰"""
        self.logger.info("å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ•°æ®å¤„ç†")
        self.logger.info(f"å›æµ‹æ—¶é—´èŒƒå›´: {self.start_date} åˆ° {self.end_date}")
        self.logger.info(f"å‚æ•°: {vars(args)}")
        
        try:
            # è¿æ¥æ•°æ®åº“
            self.logger.info("è¿æ¥æ•°æ®åº“...")
            with JuyuanDB(use_ssh_tunnel=use_ssh_tunnel) as db:
                if not db.test_connection():
                    self.logger.error("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
                    return None
                
                self.logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
                
                # ä½¿ç”¨ç³»ç»Ÿè®¾ç½®çš„å›æµ‹æ—¶é—´èŒƒå›´ï¼Œé™¤éæµ‹è¯•æ¨¡å¼
                if args.test_mode:
                    end_date = datetime.now().date()
                    start_date = end_date - timedelta(days=7)
                    query_start_date = start_date.strftime('%Y-%m-%d')
                    query_end_date = end_date.strftime('%Y-%m-%d')
                    self.logger.info(f"æµ‹è¯•æ¨¡å¼ï¼šæŸ¥è¯¢ {query_start_date} åˆ° {query_end_date} çš„æ•°æ®")
                else:
                    # ä½¿ç”¨ç³»ç»Ÿè®¾ç½®çš„å›æµ‹æ—¶é—´èŒƒå›´
                    query_start_date = self.start_date
                    query_end_date = self.end_date
                    self.logger.info(f"å›æµ‹æ¨¡å¼ï¼šæŸ¥è¯¢ {query_start_date} åˆ° {query_end_date} çš„æ•°æ®")
                
                # æ„å»ºæŸ¥è¯¢SQL
                sql = StockQueries.get_stock_quote_history(
                    start_date=query_start_date,
                    end_date=query_end_date,
                    market_codes=args.market_codes,
                    include_suspended=False
                )
                
                self.logger.info("å¼€å§‹æŸ¥è¯¢æ•°æ®...")
                df = db.read_sql(sql)
                
                if df.empty:
                    self.logger.warning("æŸ¥è¯¢ç»“æœä¸ºç©º")
                    return None
                
                self.logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œå…±è·å– {len(df)} è¡Œæ•°æ®")
                
                # æ•°æ®å¤„ç†å’Œç‰¹å¾ç”Ÿæˆ
                df = self.process_and_generate_features(df, args)
                
                # ä¿å­˜æ•°æ®
                self.save_processed_data(df, args)
                
                # ç”ŸæˆæŠ¥å‘Š
                self.generate_processing_reports(df, args)
                
                return df
                
        except Exception as e:
            self.logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return None
            
    def process_and_generate_features(self, df, args):
        """å¤„ç†å’Œç”Ÿæˆç‰¹å¾"""
        # å†…å­˜ä¼˜åŒ–
        self.logger.info("å¼€å§‹å†…å­˜ä¼˜åŒ–...")
        df = self.memory_manager.optimize_dataframe_memory(df)
        
        # æ—¶åºæ•°æ®å¤„ç†
        self.logger.info("å¼€å§‹æ—¶åºæ•°æ®å¤„ç†...")
        processor = TimeSeriesProcessor()
        
        # ä»·æ ¼å¤æƒ
        if args.adj_type != 'none':
            df = processor.adjust_prices(df, adj_type=args.adj_type)
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        if args.include_technical:
            self.logger.info("è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
            df = processor.calculate_technical_indicators(df)
            df = processor.calculate_volume_indicators(df)
            df = processor.calculate_support_resistance(df)
        
        # è®¡ç®—é‡åŒ–ç‰¹å¾
        if args.include_features:
            self.logger.info("è®¡ç®—é‡åŒ–ç‰¹å¾...")
            df = self.generate_all_features(df, args.feature_types)
            self.logger.info(f"ç‰¹å¾è®¡ç®—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(df.columns)} ä¸ªç‰¹å¾")
        
        # è¿‡æ»¤æ•°æ®
        df = processor.filter_trading_days(df)
        
        return df
        
    def generate_all_features(self, df, feature_types):
        """ç”Ÿæˆæ‰€æœ‰ç‰¹å¾"""
        try:
            # æ ‡å‡†åŒ–åˆ—å
            df_processed = df.copy()
            
            # é‡å‘½ååˆ—ä»¥åŒ¹é…ç‰¹å¾å¼•æ“æœŸæœ›çš„æ ¼å¼
            column_mapping = {
                'TradingDay': 'date',
                'OpenPrice': 'open',
                'HighPrice': 'high', 
                'LowPrice': 'low',
                'ClosePrice': 'close',
                'TurnoverVolume': 'volume',
                'TurnoverValue': 'value'
            }
            
            df_processed = df_processed.rename(columns=column_mapping)
            
            # ç”Ÿæˆä¸åŒç±»å‹çš„ç‰¹å¾
            if 'volume' in feature_types or 'price' in feature_types:
                # ä½¿ç”¨ç»Ÿä¸€çš„ç‰¹å¾å¼•æ“å¤„ç†æ‰€æœ‰ç‰¹å¾ç±»å‹
                df_processed = self.volume_engine.calculate_all_features(df_processed, feature_types)
            
            return df_processed
            
        except Exception as e:
            self.logger.error(f"ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            return df
            
    def save_processed_data(self, df, args):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if args.output_format == 'feather':
            output_file = OUTPUT_CONFIG['processed_data_dir'] / f'stock_data_{timestamp}.feather'
            df.to_feather(output_file)
        elif args.output_format == 'parquet':
            output_file = OUTPUT_CONFIG['processed_data_dir'] / f'stock_data_{timestamp}.parquet'
            df.to_parquet(output_file, compression='snappy')
        else:  # csv
            output_file = OUTPUT_CONFIG['processed_data_dir'] / f'stock_data_{timestamp}.csv'
            df.to_csv(output_file, index=False)
        
        self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
    def generate_processing_reports(self, df, args):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç”Ÿæˆæ•°æ®æŠ¥å‘Š
        data_report = self.generate_data_report(df, args)
        report_file = OUTPUT_CONFIG['reports_dir'] / f'data_report_{timestamp}.txt'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(data_report)
        
        self.logger.info(f"æ•°æ®æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ç”Ÿæˆç‰¹å¾æŠ¥å‘Š
        if args.include_features:
            feature_report = self.generate_feature_report(df, args)
            feature_report_file = OUTPUT_CONFIG['reports_dir'] / f'feature_report_{timestamp}.txt'
            
            with open(feature_report_file, 'w', encoding='utf-8') as f:
                f.write(feature_report)
            
            self.logger.info(f"ç‰¹å¾æŠ¥å‘Šå·²ä¿å­˜åˆ°: {feature_report_file}")
            
        # å†…å­˜ç›‘æ§æŠ¥å‘Š
        memory_report = self.memory_monitor.generate_memory_report()
        self.logger.info(memory_report)
        
    def start_monitoring(self, duration_minutes: int = 60):
        """å¯åŠ¨ç³»ç»Ÿç›‘æ§"""
        self.logger.info("å¯åŠ¨ç³»ç»Ÿç›‘æ§...")
        self.system_monitor.start_monitoring()
        
        try:
            import time
            time.sleep(duration_minutes * 60)
        except KeyboardInterrupt:
            self.logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ç›‘æ§")
        finally:
            self.system_monitor.stop_monitoring()
            
        # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
        summary = self.system_monitor.get_performance_summary()
        self.logger.info(f"ç›‘æ§æ‘˜è¦: {summary}")
        
    def validate_data(self, data_path: str, required_columns: list = None):
        """éªŒè¯æ•°æ®è´¨é‡"""
        self.logger.info(f"å¼€å§‹éªŒè¯æ•°æ®: {data_path}")
        
        try:
            df = pd.read_csv(data_path)
            
            # åŸºæœ¬éªŒè¯
            validation_result = self.validator.validate_dataframe(
                df, required_columns=required_columns
            )
            
            if validation_result['valid']:
                self.logger.info("æ•°æ®éªŒè¯é€šè¿‡")
            else:
                self.logger.warning("æ•°æ®éªŒè¯å¤±è´¥")
                for error in validation_result['errors']:
                    self.logger.error(f"é”™è¯¯: {error}")
                    
            # å‰ç»æ€§åå·®æ£€æŸ¥
            if 'date' in df.columns:
                lookahead_result = self.validator.check_lookahead_bias(
                    df, 'date', df.select_dtypes(include=['number']).columns.tolist()
                )
                
                if lookahead_result['valid']:
                    self.logger.info("å‰ç»æ€§åå·®æ£€æŸ¥é€šè¿‡")
                else:
                    self.logger.warning("å‘ç°å‰ç»æ€§åå·®é—®é¢˜")
                    
            return validation_result
            
        except Exception as e:
            self.logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return None
            
    def generate_features_from_file(self, data_path: str, output_path: str = None, feature_types=None):
        """ä»æ–‡ä»¶ç”Ÿæˆç‰¹å¾"""
        self.logger.info(f"å¼€å§‹ç”Ÿæˆç‰¹å¾: {data_path}")
        
        try:
            df = pd.read_csv(data_path)
            
            # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
            required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                self.logger.error(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                return None
                
            # ç”Ÿæˆç‰¹å¾
            df_with_features = self.volume_engine.calculate_all_features(df, feature_types or ['volume', 'price'])
            
            # ä¿å­˜ç»“æœ
            if output_path is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = OUTPUT_CONFIG['processed_data_dir'] / f'features_{timestamp}.csv'
                
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            df_with_features.to_csv(output_path, index=False)
            self.logger.info(f"ç‰¹å¾å·²ä¿å­˜åˆ°: {output_path}")
            
            return df_with_features
            
        except Exception as e:
            self.logger.error(f"ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            return None
            
    def generate_report(self, data_path: str, output_dir: str = None):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        self.logger.info(f"å¼€å§‹ç”ŸæˆæŠ¥å‘Š: {data_path}")
        
        if output_dir is None:
            output_dir = OUTPUT_CONFIG['reports_dir']
        
        try:
            df = pd.read_csv(data_path)
            
            # ç”Ÿæˆæ•°æ®æ‘˜è¦æŠ¥å‘Š
            data_report = self.report_generator.generate_data_summary_report(df)
            self.logger.info("æ•°æ®æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ")
            
            # ç”Ÿæˆç‰¹å¾åˆ†ææŠ¥å‘Š
            numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
            feature_report = self.report_generator.generate_feature_analysis_report(
                df, numeric_columns
            )
            self.logger.info("ç‰¹å¾åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")
            
            # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
            viz_path = self.report_generator.generate_visualization_report(
                df, numeric_columns[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
            )
            self.logger.info(f"å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {viz_path}")
            
            # ç”Ÿæˆç›¸å…³æ€§çƒ­åŠ›å›¾
            corr_path = self.report_generator.generate_correlation_heatmap(df)
            self.logger.info(f"ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ç”Ÿæˆ: {corr_path}")
            
        except Exception as e:
            self.logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            
    def detect_drift(self, reference_data_path: str, current_data_path: str):
        """æ£€æµ‹æ•°æ®æ¼‚ç§»"""
        self.logger.info("å¼€å§‹æ£€æµ‹æ•°æ®æ¼‚ç§»...")
        
        try:
            reference_df = pd.read_csv(reference_data_path)
            current_df = pd.read_csv(current_data_path)
            
            # è®¾ç½®å‚è€ƒæ•°æ®
            self.drift_detector.set_reference_data(reference_df)
            
            # æ£€æµ‹åˆ†å¸ƒæ¼‚ç§»
            drift_result = self.drift_detector.detect_distribution_drift(current_df)
            
            if drift_result['drift_detected']:
                self.logger.warning("æ£€æµ‹åˆ°åˆ†å¸ƒæ¼‚ç§»")
                self.logger.warning(f"æ¼‚ç§»ç‰¹å¾: {drift_result['drift_features']}")
            else:
                self.logger.info("æœªæ£€æµ‹åˆ°åˆ†å¸ƒæ¼‚ç§»")
                
            # æ£€æµ‹åå˜é‡æ¼‚ç§»
            covariate_result = self.drift_detector.detect_covariate_drift(current_df)
            
            if covariate_result['drift_detected']:
                self.logger.warning("æ£€æµ‹åˆ°åå˜é‡æ¼‚ç§»")
                self.logger.warning(f"æ¼‚ç§»åˆ†æ•°: {covariate_result['drift_score']}")
            else:
                self.logger.info("æœªæ£€æµ‹åˆ°åå˜é‡æ¼‚ç§»")
                
            return drift_result, covariate_result
            
        except Exception as e:
            self.logger.error(f"æ¼‚ç§»æ£€æµ‹å¤±è´¥: {e}")
            return None, None
            
    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        self.logger.info("å¼€å§‹å†…å­˜ä¼˜åŒ–...")
        
        before_usage = self.memory_manager.get_memory_usage()
        self.logger.info(f"ä¼˜åŒ–å‰å†…å­˜ä½¿ç”¨: {before_usage['process_memory_mb']:.2f} MB")
        
        optimization_result = self.memory_manager.optimize_memory()
        
        after_usage = optimization_result['after_usage']
        self.logger.info(f"ä¼˜åŒ–åå†…å­˜ä½¿ç”¨: {after_usage['process_memory_mb']:.2f} MB")
        self.logger.info(f"é‡Šæ”¾å†…å­˜: {optimization_result['freed_mb']:.2f} MB")
        
        return optimization_result
        
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        self.logger.info("ç³»ç»Ÿä¿¡æ¯:")
        
        # ç³»ç»ŸåŸºæœ¬ä¿¡æ¯
        import psutil
        system_info = {
            'CPUæ ¸å¿ƒæ•°': psutil.cpu_count(),
            'å†…å­˜æ€»é‡': f"{psutil.virtual_memory().total / (1024**3):.2f} GB",
            'ç£ç›˜æ€»é‡': f"{psutil.disk_usage('/').total / (1024**3):.2f} GB",
            'Pythonç‰ˆæœ¬': sys.version,
            'é¡¹ç›®è·¯å¾„': str(project_root)
        }
        
        for key, value in system_info.items():
            self.logger.info(f"  {key}: {value}")
            
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_usage = self.memory_manager.get_memory_usage()
        self.logger.info(f"å½“å‰å†…å­˜ä½¿ç”¨: {memory_usage['process_memory_mb']:.2f} MB")
        
        # å†…å­˜å‹åŠ›è¯„ä¼°
        pressure = self.memory_manager.check_memory_pressure()
        self.logger.info(f"å†…å­˜å‹åŠ›ç­‰çº§: {pressure['pressure_level']}")
        self.logger.info(f"å»ºè®®: {pressure['recommendation']}")
        
    def generate_data_report(self, df, args):
        """ç”Ÿæˆæ•°æ®æŠ¥å‘Š"""
        report = f"""
è‚¡ç¥¨æ•°æ®å¤„ç†æŠ¥å‘Š
================

å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
å›æµ‹æ—¶é—´èŒƒå›´: {self.start_date} åˆ° {self.end_date}
æŸ¥è¯¢å‚æ•°:
- å¸‚åœºä»£ç : {args.market_codes}
- å¤æƒç±»å‹: {args.adj_type}
- æŠ€æœ¯æŒ‡æ ‡: {'æ˜¯' if args.include_technical else 'å¦'}
- é‡åŒ–ç‰¹å¾: {'æ˜¯' if args.include_features else 'å¦'}
- ç‰¹å¾ç±»å‹: {args.feature_types if args.include_features else 'æ— '}

æ•°æ®ç»Ÿè®¡:
- æ€»è®°å½•æ•°: {len(df):,}
- è‚¡ç¥¨æ•°é‡: {df['SecuCode'].nunique():,}
- äº¤æ˜“æ—¥æœŸèŒƒå›´: {df['TradingDay'].min()} åˆ° {df['TradingDay'].max()}
- æ•°æ®åˆ—æ•°: {len(df.columns)}

æ•°æ®åˆ—ä¿¡æ¯:
"""
        
        for col in df.columns:
            report += f"- {col}: {df[col].dtype}\n"
        
        if 'ClosePrice' in df.columns:
            report += f"""
ä»·æ ¼ç»Ÿè®¡:
- æ”¶ç›˜ä»·èŒƒå›´: {df['ClosePrice'].min():.2f} - {df['ClosePrice'].max():.2f}
- å¹³å‡æ”¶ç›˜ä»·: {df['ClosePrice'].mean():.2f}
- æ”¶ç›˜ä»·æ ‡å‡†å·®: {df['ClosePrice'].std():.2f}
"""
        
        if 'TurnoverVolume' in df.columns:
            report += f"""
æˆäº¤é‡ç»Ÿè®¡:
- æˆäº¤é‡èŒƒå›´: {df['TurnoverVolume'].min():,.0f} - {df['TurnoverVolume'].max():,.0f}
- å¹³å‡æˆäº¤é‡: {df['TurnoverVolume'].mean():,.0f}
- æ€»æˆäº¤é‡: {df['TurnoverVolume'].sum():,.0f}
"""
        
        return report

    def generate_feature_report(self, df, args):
        """ç”Ÿæˆç‰¹å¾æŠ¥å‘Š"""
        # è·å–æ–°å¢çš„ç‰¹å¾åˆ—
        original_columns = {
            'SecuCode', 'SecuMarket', 'Ifsuspend', 'TradingDay', 
            'PrevClosePrice', 'OpenPrice', 'HighPrice', 'LowPrice', 
            'ClosePrice', 'TurnoverVolume', 'TurnoverValue', 
            'PriceCeiling', 'PriceFloor'
        }
        
        feature_columns = [col for col in df.columns if col not in original_columns]
        
        report = f"""
é‡åŒ–ç‰¹å¾æŠ¥å‘Š
============

ç‰¹å¾è®¡ç®—æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç‰¹å¾ç±»å‹: {args.feature_types}
æ€»ç‰¹å¾æ•°: {len(feature_columns)}

ç‰¹å¾åˆ†ç±»ç»Ÿè®¡:
"""
        
        # æŒ‰ç‰¹å¾ç±»å‹åˆ†ç±»ç»Ÿè®¡
        feature_categories = {
            'ä»·æ ¼ç‰¹å¾': [col for col in feature_columns if any(x in col for x in ['Return', 'Momentum', 'Vol', 'SMA', 'EMA', 'BB', 'Support', 'Resistance', 'Fib'])],
            'æˆäº¤é‡ç‰¹å¾': [col for col in feature_columns if any(x in col for x in ['Volume', 'VWAP', 'MFI'])],
            'æŠ€æœ¯æŒ‡æ ‡': [col for col in feature_columns if any(x in col for x in ['RSI', 'MACD', 'Stoch', 'Williams', 'Hammer', 'Doji', 'Engulfing'])],
            'å­£èŠ‚æ€§ç‰¹å¾': [col for col in feature_columns if any(x in col for x in ['DayOfWeek', 'Month', 'Quarter', 'Seasonality'])]
        }
        
        for category, features in feature_categories.items():
            report += f"- {category}: {len(features)} ä¸ª\n"
        
        report += f"""
ç‰¹å¾åˆ—è¡¨:
"""
        
        for category, features in feature_categories.items():
            if features:
                report += f"\n{category}:\n"
                for feature in sorted(features):
                    report += f"  - {feature}\n"
        
        # ç‰¹å¾è´¨é‡ç»Ÿè®¡
        report += f"""
ç‰¹å¾è´¨é‡ç»Ÿè®¡:
- ç¼ºå¤±å€¼æ¯”ä¾‹ > 50% çš„ç‰¹å¾: {len([col for col in feature_columns if df[col].isna().sum() / len(df) > 0.5])} ä¸ª
- é›¶æ–¹å·®ç‰¹å¾: {len([col for col in feature_columns if df[col].std() == 0])} ä¸ª
- å¸¸æ•°ç‰¹å¾: {len([col for col in feature_columns if df[col].nunique() == 1])} ä¸ª

ç‰¹å¾ç›¸å…³æ€§åˆ†æ:
- é«˜åº¦ç›¸å…³ç‰¹å¾å¯¹ (>0.95): éœ€è¦è¿›ä¸€æ­¥åˆ†æ
- å»ºè®®è¿›è¡Œç‰¹å¾é€‰æ‹©ä»¥é™ä½ç»´åº¦
"""
        
        return report

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='é‡åŒ–ç‰¹å¾ç³»ç»Ÿ')
    
    # å…¨å±€é€‰é¡¹
    parser.add_argument('--use-ssh-tunnel', action='store_true', help='ä½¿ç”¨SSHéš§é“è¿æ¥æ•°æ®åº“')
    parser.add_argument('--test-connection', action='store_true', help='æµ‹è¯•æ•°æ®åº“è¿æ¥')
    parser.add_argument('--start-date', type=str, default=DATA_CONFIG.get('default_start_date', '2024-01-01'), 
                       help='å›æµ‹å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=DATA_CONFIG.get('default_end_date', '2024-12-31'), 
                       help='å›æµ‹ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ•°æ®å¤„ç†å‘½ä»¤
    process_parser = subparsers.add_parser('process', help='å¤„ç†è‚¡ç¥¨æ•°æ®')
    process_parser.add_argument('--market-codes', type=int, nargs='+', default=DATA_CONFIG['market_codes'], help='å¸‚åœºä»£ç åˆ—è¡¨')
    process_parser.add_argument('--output-format', type=str, default=DATA_CONFIG['output_format'], choices=['feather', 'parquet', 'csv'], help='è¾“å‡ºæ ¼å¼')
    process_parser.add_argument('--chunk-size', type=int, default=DATA_CONFIG['chunk_size'], help='åˆ†å—å¤§å°')
    process_parser.add_argument('--adj-type', type=str, default='forward', choices=['forward', 'backward', 'none'], help='å¤æƒç±»å‹')
    process_parser.add_argument('--include-technical', action='store_true', help='æ˜¯å¦è®¡ç®—æŠ€æœ¯æŒ‡æ ‡')
    process_parser.add_argument('--include-features', action='store_true', help='æ˜¯å¦è®¡ç®—é‡åŒ–ç‰¹å¾')
    process_parser.add_argument('--feature-types', type=str, nargs='+', default=['price', 'volume', 'technical'], choices=['price', 'volume', 'technical', 'cross_asset'], help='è¦è®¡ç®—çš„ç‰¹å¾ç±»å‹')
    process_parser.add_argument('--test-mode', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å°‘é‡æ•°æ®ï¼‰')
    
    # ç›‘æ§å‘½ä»¤
    monitor_parser = subparsers.add_parser('monitor', help='å¯åŠ¨ç³»ç»Ÿç›‘æ§')
    monitor_parser.add_argument('--duration', type=int, default=60, help='ç›‘æ§æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰')
    
    # éªŒè¯å‘½ä»¤
    validate_parser = subparsers.add_parser('validate', help='éªŒè¯æ•°æ®è´¨é‡')
    validate_parser.add_argument('--data', type=str, required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    validate_parser.add_argument('--columns', nargs='+', help='å¿…éœ€åˆ—å')
    
    # ç‰¹å¾ç”Ÿæˆå‘½ä»¤
    features_parser = subparsers.add_parser('features', help='ä»æ–‡ä»¶ç”Ÿæˆç‰¹å¾')
    features_parser.add_argument('--data', type=str, required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    features_parser.add_argument('--output', type=str, help='è¾“å‡ºè·¯å¾„')
    features_parser.add_argument('--feature-types', type=str, nargs='+', default=['volume', 'price'], choices=['volume', 'price', 'technical', 'cross_asset'], help='ç‰¹å¾ç±»å‹')
    
    # æŠ¥å‘Šç”Ÿæˆå‘½ä»¤
    report_parser = subparsers.add_parser('report', help='ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    report_parser.add_argument('--data', type=str, required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    report_parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')
    
    # æ¼‚ç§»æ£€æµ‹å‘½ä»¤
    drift_parser = subparsers.add_parser('drift', help='æ£€æµ‹æ•°æ®æ¼‚ç§»')
    drift_parser.add_argument('--reference', type=str, required=True, help='å‚è€ƒæ•°æ®è·¯å¾„')
    drift_parser.add_argument('--current', type=str, required=True, help='å½“å‰æ•°æ®è·¯å¾„')
    
    # å†…å­˜ä¼˜åŒ–å‘½ä»¤
    optimize_parser = subparsers.add_parser('optimize', help='ä¼˜åŒ–å†…å­˜ä½¿ç”¨')
    
    # ç³»ç»Ÿä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser('info', help='æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if args.test_connection:
        print("ğŸ” æµ‹è¯•æ•°æ®åº“è¿æ¥...")
        from database.connector import test_database_connection, get_database_info
        
        # æµ‹è¯•ç›´æ¥è¿æ¥
        print("\n1. æµ‹è¯•ç›´æ¥è¿æ¥...")
        if test_database_connection(use_ssh_tunnel=False):
            info = get_database_info(use_ssh_tunnel=False)
            print(f"âœ… ç›´æ¥è¿æ¥æˆåŠŸ: {info}")
        else:
            print("âŒ ç›´æ¥è¿æ¥å¤±è´¥")
        
        # æµ‹è¯•SSHéš§é“è¿æ¥
        print("\n2. æµ‹è¯•SSHéš§é“è¿æ¥...")
        if test_database_connection(use_ssh_tunnel=True):
            info = get_database_info(use_ssh_tunnel=True)
            print(f"âœ… SSHéš§é“è¿æ¥æˆåŠŸ: {info}")
        else:
            print("âŒ SSHéš§é“è¿æ¥å¤±è´¥")
        
        return 0
    
    if not args.command:
        print("è¯·æŒ‡å®šè¦æ‰§è¡Œçš„å‘½ä»¤ã€‚ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯ã€‚")
        return 1
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°çš„å›æµ‹æ—¶é—´èŒƒå›´
    system = QuantFeatureSystem(
        start_date=args.start_date,
        end_date=args.end_date
    )
    
    try:
        if args.command == 'process':
            # ä¼ é€’SSHéš§é“å‚æ•°
            system.process_stock_data(args, use_ssh_tunnel=args.use_ssh_tunnel)
            
        elif args.command == 'monitor':
            system.start_monitoring(args.duration)
            
        elif args.command == 'validate':
            system.validate_data(args.data, args.columns)
            
        elif args.command == 'features':
            system.generate_features_from_file(args.data, args.output, args.feature_types)
            
        elif args.command == 'report':
            system.generate_report(args.data, args.output)
            
        elif args.command == 'drift':
            system.detect_drift(args.reference, args.current)
            
        elif args.command == 'optimize':
            system.optimize_memory()
            
        elif args.command == 'info':
            system.show_system_info()
            
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        system.logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 